{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e3253a",
   "metadata": {},
   "source": [
    "# Implementando GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1348f0",
   "metadata": {},
   "source": [
    "## Implementando com a mesma arquitetura do modelo SE3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22764592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 22:43:02.807374: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-01 22:43:04.653016: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-01 22:43:04.653097: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-01 22:43:04.658903: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-01 22:43:05.437373: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-01 22:43:12.777148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM,Bidirectional,Dropout\n",
    "from keras.layers import AveragePooling1D\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.layers import Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import * \n",
    "from sklearn.model_selection import KFold \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab165edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7786167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reqTxt.csv', header=None) #Report or file containing the set of training and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618b243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add ca against object literals in function inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>update branding for appcelerator plugin to app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>create new json schema for sdk teamcreate json...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>create project references property pagecreate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new desktop project wizarddesktop need to conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23308</th>\n",
       "      <td>introduce datastax java driver in effort to mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23309</th>\n",
       "      <td>simplify central sso eliminate ext token valid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23310</th>\n",
       "      <td>proof of concept use akka for unique value enf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23311</th>\n",
       "      <td>set a property to turn off usergrids dependenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23312</th>\n",
       "      <td>allow usergridapiurlbase to be set when creati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23313 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      add ca against object literals in function inv...\n",
       "1      update branding for appcelerator plugin to app...\n",
       "2      create new json schema for sdk teamcreate json...\n",
       "3      create project references property pagecreate ...\n",
       "4      new desktop project wizarddesktop need to conv...\n",
       "...                                                  ...\n",
       "23308  introduce datastax java driver in effort to mi...\n",
       "23309  simplify central sso eliminate ext token valid...\n",
       "23310  proof of concept use akka for unique value enf...\n",
       "23311  set a property to turn off usergrids dependenc...\n",
       "23312  allow usergridapiurlbase to be set when creati...\n",
       "\n",
       "[23313 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f00c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRequire = df.iloc[:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196cba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23313, 1)\n",
      "Int64Index([0], dtype='int64')\n",
      "add ca against object literals in function invocationsthe idea here is that if our metadata captures a type as function arg we should be able to create an instance of that type as an object literal as an arg to a function invocation for examplep    tiuicreatelabel  ltpropertycaheregt  code prediv\n",
      "23313\n",
      "Train and test dataset loaded...\n"
     ]
    }
   ],
   "source": [
    "print(dfRequire.shape)\n",
    "print(dfRequire.columns)\n",
    "X = dfRequire[0]\n",
    "print(X[0])\n",
    "X = np.array(X)\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "print('Train and test dataset loaded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c978d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (23313, 1)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv('estiDeep.data', header=None) #File containing the set of training and test labels.\n",
    "y = np.array(y)\n",
    "print ('Shape of label tensor:', y.shape)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f9ea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=1000, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "#Number of texts in train and test dataset \n",
    "MAX_LEN = 23313\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1000) \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "print(kf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a54324aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in /home/daniel/anaconda3/lib/python3.11/site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/daniel/anaconda3/lib/python3.11/site-packages (from tensorflow-hub) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/daniel/anaconda3/lib/python3.11/site-packages (from tensorflow-hub) (4.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59d50302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30aed038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49a1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(X)\n",
    "seq = token.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb601b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_seq = pad_sequences(seq,maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e7cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(token.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d8dec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8baff036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:31, 12616.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embedding_vector = {}\n",
    "f = open(os.path.join('GloVe/glove.6B.300d.txt'))\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6162abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96952/96952 [00:00<00:00, 400319.51it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,300))\n",
    "for word,i in tqdm(token.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d3a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89802 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20981, 300)\n",
      "Shape of data test tensor: (2332, 300)\n",
      "train_y: (20981, 1)\n",
      "test_y: (2332, 1)\n",
      "(None, 1, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d (Average  (None, 1, 300)            0         \n",
      " Pooling1D)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                70200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 11s 33ms/step - loss: 116.4603 - mae: 4.9404 - val_loss: 97.6380 - val_mae: 4.5109\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 102.2004 - mae: 4.7012 - val_loss: 93.9314 - val_mae: 4.4870\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 100.3311 - mae: 4.7809 - val_loss: 92.8981 - val_mae: 4.6849\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 99.3055 - mae: 4.8162 - val_loss: 92.3731 - val_mae: 4.5822\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 98.7441 - mae: 4.8107 - val_loss: 92.0735 - val_mae: 4.6690\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 98.3177 - mae: 4.7885 - val_loss: 91.7508 - val_mae: 4.6789\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 97.8308 - mae: 4.8225 - val_loss: 91.5509 - val_mae: 4.5102\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 97.5619 - mae: 4.7775 - val_loss: 91.1503 - val_mae: 4.6191\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 97.1703 - mae: 4.7971 - val_loss: 90.9303 - val_mae: 4.7279\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 96.8198 - mae: 4.7622 - val_loss: 90.8471 - val_mae: 4.8500\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.5066 - mae: 4.7814 - val_loss: 90.2293 - val_mae: 4.4775\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 96.0206 - mae: 4.7613 - val_loss: 89.7242 - val_mae: 4.6254\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 95.9514 - mae: 4.7530 - val_loss: 89.4407 - val_mae: 4.5473\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - 5s 32ms/step - loss: 95.4477 - mae: 4.7376 - val_loss: 89.0573 - val_mae: 4.5320\n",
      "Epoch 15/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.1944 - mae: 4.7435 - val_loss: 88.7994 - val_mae: 4.4212\n",
      "Epoch 16/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.6358 - mae: 4.7186 - val_loss: 88.3274 - val_mae: 4.4744\n",
      "Epoch 17/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 94.0212 - mae: 4.7177 - val_loss: 88.4395 - val_mae: 4.8411\n",
      "Epoch 18/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 93.9341 - mae: 4.6989 - val_loss: 87.7862 - val_mae: 4.3821\n",
      "Epoch 19/30\n",
      "164/164 [==============================] - 5s 32ms/step - loss: 93.0779 - mae: 4.6872 - val_loss: 87.2239 - val_mae: 4.4636\n",
      "Epoch 20/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.0012 - mae: 4.7003 - val_loss: 87.1120 - val_mae: 4.4049\n",
      "Epoch 21/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 92.7685 - mae: 4.6530 - val_loss: 86.8817 - val_mae: 4.5491\n",
      "Epoch 22/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 92.4798 - mae: 4.6605 - val_loss: 86.5509 - val_mae: 4.4635\n",
      "Epoch 23/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 91.8478 - mae: 4.6575 - val_loss: 86.5225 - val_mae: 4.5043\n",
      "Epoch 24/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 91.4302 - mae: 4.6331 - val_loss: 86.6057 - val_mae: 4.3759\n",
      "Epoch 25/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 91.3199 - mae: 4.6318 - val_loss: 86.3459 - val_mae: 4.3242\n",
      "Epoch 26/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 90.8024 - mae: 4.6293 - val_loss: 85.8021 - val_mae: 4.6101\n",
      "Epoch 27/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 90.2927 - mae: 4.5969 - val_loss: 85.6704 - val_mae: 4.3711\n",
      "Epoch 28/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 90.2982 - mae: 4.6279 - val_loss: 86.6142 - val_mae: 4.1919\n",
      "Epoch 29/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 89.8977 - mae: 4.6190 - val_loss: 85.2158 - val_mae: 4.4564\n",
      "Epoch 30/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 89.7424 - mae: 4.6097 - val_loss: 85.2575 - val_mae: 4.6961\n",
      "\n",
      "\n",
      "MAE: 4.696073\n",
      "MedAE: 3.204197\n",
      "r2: 0.090865\n",
      "MSE: 85.257469\n",
      "maxrror: 94.704267\n",
      "Concluido 1\n",
      "Found 89519 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20981, 300)\n",
      "Shape of data test tensor: (2332, 300)\n",
      "train_y: (20981, 1)\n",
      "test_y: (2332, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_1 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 10s 32ms/step - loss: 117.4516 - mae: 5.0372 - val_loss: 109.4081 - val_mae: 4.6066\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 101.5806 - mae: 4.6835 - val_loss: 107.0858 - val_mae: 4.7691\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 99.5002 - mae: 4.7701 - val_loss: 106.0546 - val_mae: 4.8676\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 98.4663 - mae: 4.7940 - val_loss: 104.9984 - val_mae: 4.7290\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 97.5830 - mae: 4.8015 - val_loss: 103.9614 - val_mae: 4.8769\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 96.9436 - mae: 4.8148 - val_loss: 103.3323 - val_mae: 4.7613\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.5110 - mae: 4.7886 - val_loss: 102.8503 - val_mae: 4.8791\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 96.1078 - mae: 4.7931 - val_loss: 102.4761 - val_mae: 4.7303\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.4431 - mae: 4.7982 - val_loss: 101.9606 - val_mae: 4.6905\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.1374 - mae: 4.7631 - val_loss: 101.5132 - val_mae: 4.9159\n",
      "Epoch 11/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 93.5834 - mae: 4.7600Restoring model weights from the end of the best epoch: 1.\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 94.5558 - mae: 4.7700 - val_loss: 100.9858 - val_mae: 4.8748\n",
      "Epoch 11: early stopping\n",
      "\n",
      "\n",
      "MAE: 4.606585\n",
      "MedAE: 2.663517\n",
      "r2: -0.026083\n",
      "MSE: 109.408150\n",
      "maxrror: 97.576008\n",
      "Concluido 2\n",
      "Found 90065 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20981, 300)\n",
      "Shape of data test tensor: (2332, 300)\n",
      "train_y: (20981, 1)\n",
      "test_y: (2332, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_2 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 10s 33ms/step - loss: 115.8484 - mae: 4.9965 - val_loss: 102.2612 - val_mae: 4.6992\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 101.6272 - mae: 4.7006 - val_loss: 99.1274 - val_mae: 4.6590\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 100.1004 - mae: 4.7768 - val_loss: 98.3012 - val_mae: 4.7076\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 99.2777 - mae: 4.8025 - val_loss: 97.8138 - val_mae: 4.9483\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 98.7198 - mae: 4.7954 - val_loss: 97.3310 - val_mae: 4.8566\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 98.3037 - mae: 4.8055 - val_loss: 97.1379 - val_mae: 4.6814\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 98.0348 - mae: 4.8033 - val_loss: 96.7897 - val_mae: 4.6938\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 97.6617 - mae: 4.7782 - val_loss: 96.3866 - val_mae: 4.8149\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 97.2341 - mae: 4.7878 - val_loss: 96.1009 - val_mae: 4.7576\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 97.2108 - mae: 4.7733 - val_loss: 95.6996 - val_mae: 4.7832\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 96.6699 - mae: 4.7759 - val_loss: 95.2543 - val_mae: 4.7380\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 96.5188 - mae: 4.7421Restoring model weights from the end of the best epoch: 2.\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.5188 - mae: 4.7421 - val_loss: 95.2048 - val_mae: 4.9583\n",
      "Epoch 12: early stopping\n",
      "\n",
      "\n",
      "MAE: 4.658975\n",
      "MedAE: 3.102409\n",
      "r2: 0.000977\n",
      "MSE: 99.127430\n",
      "maxrror: 95.228189\n",
      "Concluido 3\n",
      "Found 90531 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20982, 300)\n",
      "Shape of data test tensor: (2331, 300)\n",
      "train_y: (20982, 1)\n",
      "test_y: (2331, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_3 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 10s 34ms/step - loss: 114.7460 - mae: 4.9609 - val_loss: 114.3856 - val_mae: 4.9174\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 100.6202 - mae: 4.6628 - val_loss: 109.9304 - val_mae: 5.0289\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 99.0849 - mae: 4.7478 - val_loss: 108.8078 - val_mae: 4.8440\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 98.2271 - mae: 4.7510 - val_loss: 107.5676 - val_mae: 5.0321\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 97.7477 - mae: 4.7494 - val_loss: 107.0291 - val_mae: 4.9519\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 97.4152 - mae: 4.7619 - val_loss: 106.6493 - val_mae: 4.9498\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 97.1302 - mae: 4.7535 - val_loss: 106.4896 - val_mae: 4.8609\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.9204 - mae: 4.7600 - val_loss: 106.0672 - val_mae: 4.9035\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 96.5911 - mae: 4.7582 - val_loss: 105.7478 - val_mae: 4.9048\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.5605 - mae: 4.7525 - val_loss: 105.4810 - val_mae: 4.9599\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.2364 - mae: 4.7541 - val_loss: 105.9169 - val_mae: 4.7431\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.1082 - mae: 4.7485 - val_loss: 105.0117 - val_mae: 4.8546\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 95.9335 - mae: 4.7549 - val_loss: 104.6749 - val_mae: 4.8601\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 95.6594 - mae: 4.7358 - val_loss: 104.4248 - val_mae: 4.8150\n",
      "Epoch 15/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.4412 - mae: 4.7243 - val_loss: 103.6430 - val_mae: 4.9994\n",
      "Epoch 16/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.1817 - mae: 4.7268 - val_loss: 103.7428 - val_mae: 4.8623\n",
      "Epoch 17/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.0050 - mae: 4.7110 - val_loss: 102.9865 - val_mae: 4.8819\n",
      "Epoch 18/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.6785 - mae: 4.7100 - val_loss: 102.8187 - val_mae: 4.9398\n",
      "Epoch 19/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 94.7691 - mae: 4.6984 - val_loss: 103.1806 - val_mae: 4.6745\n",
      "Epoch 20/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.4228 - mae: 4.7005 - val_loss: 102.4227 - val_mae: 4.7857\n",
      "Epoch 21/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.0298 - mae: 4.6961 - val_loss: 102.1534 - val_mae: 4.7562\n",
      "Epoch 22/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.4662 - mae: 4.6755 - val_loss: 101.5190 - val_mae: 4.7868\n",
      "Epoch 23/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.3927 - mae: 4.6624 - val_loss: 101.1957 - val_mae: 4.8158\n",
      "Epoch 24/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.3623 - mae: 4.6711 - val_loss: 100.3923 - val_mae: 4.7942\n",
      "Epoch 25/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 92.9114 - mae: 4.6423 - val_loss: 100.6147 - val_mae: 4.7534\n",
      "Epoch 26/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 92.7325 - mae: 4.6561 - val_loss: 100.0163 - val_mae: 4.8085\n",
      "Epoch 27/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 92.3276 - mae: 4.6416 - val_loss: 100.0984 - val_mae: 4.6554\n",
      "Epoch 28/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 92.2496 - mae: 4.6441 - val_loss: 99.9556 - val_mae: 4.6557\n",
      "Epoch 29/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 92.3571 - mae: 4.6381 - val_loss: 99.4785 - val_mae: 5.0353\n",
      "Epoch 30/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 91.3658 - mae: 4.6215 - val_loss: 98.9392 - val_mae: 4.7921\n",
      "\n",
      "\n",
      "MAE: 4.792057\n",
      "MedAE: 3.010405\n",
      "r2: 0.093063\n",
      "MSE: 98.939190\n",
      "maxrror: 96.461895\n",
      "Concluido 4\n",
      "Found 89717 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20982, 300)\n",
      "Shape of data test tensor: (2331, 300)\n",
      "train_y: (20982, 1)\n",
      "test_y: (2331, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_4 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 10s 32ms/step - loss: 138.4751 - mae: 6.1487 - val_loss: 131.2080 - val_mae: 5.9583\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 136.5374 - mae: 5.9896 - val_loss: 129.3324 - val_mae: 5.7988\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 134.6618 - mae: 5.8319 - val_loss: 127.5271 - val_mae: 5.6410\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 132.8574 - mae: 5.6756 - val_loss: 125.7999 - val_mae: 5.4857\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 131.1261 - mae: 5.5199 - val_loss: 124.1354 - val_mae: 5.3319\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 129.4619 - mae: 5.3681 - val_loss: 122.5395 - val_mae: 5.1800\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 127.8620 - mae: 5.2257 - val_loss: 121.0073 - val_mae: 5.0612\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 6s 37ms/step - loss: 126.3264 - mae: 5.1252 - val_loss: 119.5298 - val_mae: 4.9659\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 32ms/step - loss: 124.8515 - mae: 5.0308 - val_loss: 118.1196 - val_mae: 4.8721\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 123.4371 - mae: 4.9379 - val_loss: 116.7699 - val_mae: 4.7795\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 122.0800 - mae: 4.8458 - val_loss: 115.4678 - val_mae: 4.6873\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 120.7765 - mae: 4.7555 - val_loss: 114.2225 - val_mae: 4.5962\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 119.5269 - mae: 4.6647 - val_loss: 113.0304 - val_mae: 4.5061\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 118.3290 - mae: 4.5849 - val_loss: 111.8947 - val_mae: 4.4449\n",
      "Epoch 15/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 117.1827 - mae: 4.5344 - val_loss: 110.7998 - val_mae: 4.3972\n",
      "Epoch 16/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 116.0865 - mae: 4.4870 - val_loss: 109.7567 - val_mae: 4.3502\n",
      "Epoch 17/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 115.0381 - mae: 4.4399 - val_loss: 108.7641 - val_mae: 4.3037\n",
      "Epoch 18/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 114.0368 - mae: 4.3932 - val_loss: 107.8125 - val_mae: 4.2575\n",
      "Epoch 19/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 113.0824 - mae: 4.3477 - val_loss: 106.9192 - val_mae: 4.2124\n",
      "Epoch 20/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 112.1733 - mae: 4.3020 - val_loss: 106.0574 - val_mae: 4.1672\n",
      "Epoch 21/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 111.3075 - mae: 4.2587 - val_loss: 105.2403 - val_mae: 4.1327\n",
      "Epoch 22/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 110.4831 - mae: 4.2451 - val_loss: 104.4659 - val_mae: 4.1336\n",
      "Epoch 23/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 109.7013 - mae: 4.2438 - val_loss: 103.7286 - val_mae: 4.1344\n",
      "Epoch 24/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 108.9607 - mae: 4.2424 - val_loss: 103.0382 - val_mae: 4.1352\n",
      "Epoch 25/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 108.2603 - mae: 4.2421 - val_loss: 102.3902 - val_mae: 4.1362\n",
      "Epoch 26/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 107.6005 - mae: 4.2416 - val_loss: 101.7758 - val_mae: 4.1371\n",
      "Epoch 27/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 106.9769 - mae: 4.2405 - val_loss: 101.1967 - val_mae: 4.1375\n",
      "Epoch 28/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 106.3906 - mae: 4.2397 - val_loss: 100.6542 - val_mae: 4.1376\n",
      "Epoch 29/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 105.8345 - mae: 4.2366 - val_loss: 100.1428 - val_mae: 4.1364\n",
      "Epoch 30/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 105.3157 - mae: 4.2405 - val_loss: 99.6703 - val_mae: 4.1441\n",
      "\n",
      "\n",
      "MAE: 4.144142\n",
      "MedAE: 2.124015\n",
      "r2: -0.041415\n",
      "MSE: 99.670333\n",
      "maxrror: 95.875985\n",
      "Concluido 5\n",
      "Found 89516 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20982, 300)\n",
      "Shape of data test tensor: (2331, 300)\n",
      "train_y: (20982, 1)\n",
      "test_y: (2331, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_5 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 10s 33ms/step - loss: 111.4172 - mae: 4.8501 - val_loss: 117.3544 - val_mae: 4.8745\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 98.6599 - mae: 4.7102 - val_loss: 115.4216 - val_mae: 5.0029\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 97.4918 - mae: 4.7576 - val_loss: 114.2565 - val_mae: 5.1757\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.9061 - mae: 4.7755 - val_loss: 113.3752 - val_mae: 5.1842\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.5154 - mae: 4.7625 - val_loss: 113.6031 - val_mae: 4.8390\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.9730 - mae: 4.7732 - val_loss: 113.0536 - val_mae: 4.8179\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 95.6612 - mae: 4.7416 - val_loss: 111.8085 - val_mae: 4.9829\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.1322 - mae: 4.7340 - val_loss: 110.9017 - val_mae: 5.0676\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.8836 - mae: 4.7434 - val_loss: 111.0581 - val_mae: 4.8910\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.5360 - mae: 4.7310 - val_loss: 110.7734 - val_mae: 4.8653\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 94.0427 - mae: 4.7139 - val_loss: 109.7838 - val_mae: 4.9247\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.4910 - mae: 4.7107 - val_loss: 109.2873 - val_mae: 5.0239\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.0629 - mae: 4.6972 - val_loss: 108.0761 - val_mae: 5.2106\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 93.2708 - mae: 4.6880 - val_loss: 110.5869 - val_mae: 4.6289\n",
      "Epoch 15/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 92.3608 - mae: 4.6790 - val_loss: 107.4489 - val_mae: 5.0490\n",
      "Epoch 16/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 92.0962 - mae: 4.6602 - val_loss: 108.6612 - val_mae: 4.7020\n",
      "Epoch 17/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 91.9126 - mae: 4.6768 - val_loss: 108.9043 - val_mae: 4.6241\n",
      "Epoch 18/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 91.4561 - mae: 4.6285 - val_loss: 106.3719 - val_mae: 4.9522\n",
      "Epoch 19/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 90.6497 - mae: 4.6390 - val_loss: 106.5870 - val_mae: 4.7952\n",
      "Epoch 20/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 90.8601 - mae: 4.6407 - val_loss: 106.5610 - val_mae: 4.8960\n",
      "Epoch 21/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 90.1998 - mae: 4.6220 - val_loss: 106.0264 - val_mae: 4.7220\n",
      "Epoch 22/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 90.0600 - mae: 4.6035 - val_loss: 105.3033 - val_mae: 4.8848\n",
      "Epoch 23/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 89.4087 - mae: 4.5903 - val_loss: 104.8207 - val_mae: 4.8464\n",
      "Epoch 24/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 89.7566 - mae: 4.6091 - val_loss: 105.5670 - val_mae: 4.7021\n",
      "Epoch 25/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 88.7003 - mae: 4.5800 - val_loss: 104.5829 - val_mae: 4.6802\n",
      "Epoch 26/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 88.7406 - mae: 4.5909 - val_loss: 103.4343 - val_mae: 4.8565\n",
      "Epoch 27/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 88.3484 - mae: 4.5882Restoring model weights from the end of the best epoch: 17.\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 88.3484 - mae: 4.5882 - val_loss: 104.2164 - val_mae: 4.7293\n",
      "Epoch 27: early stopping\n",
      "\n",
      "\n",
      "MAE: 4.624129\n",
      "MedAE: 2.747799\n",
      "r2: 0.061158\n",
      "MSE: 108.904334\n",
      "maxrror: 94.968069\n",
      "Concluido 6\n",
      "Found 89090 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20982, 300)\n",
      "Shape of data test tensor: (2331, 300)\n",
      "train_y: (20982, 1)\n",
      "test_y: (2331, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_6 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 10s 32ms/step - loss: 117.1654 - mae: 5.0360 - val_loss: 115.9830 - val_mae: 4.8120\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 101.0674 - mae: 4.6535 - val_loss: 111.1596 - val_mae: 5.0001\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 99.2454 - mae: 4.7298 - val_loss: 109.5618 - val_mae: 4.9938\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 98.3366 - mae: 4.7780 - val_loss: 108.3699 - val_mae: 4.9677\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 97.7279 - mae: 4.7824 - val_loss: 107.7375 - val_mae: 4.9441\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 97.2829 - mae: 4.7794 - val_loss: 107.5178 - val_mae: 4.7793\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.8195 - mae: 4.7658 - val_loss: 106.8041 - val_mae: 4.9716\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 96.5624 - mae: 4.7904 - val_loss: 106.6991 - val_mae: 4.8791\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 96.2888 - mae: 4.7517 - val_loss: 106.5436 - val_mae: 5.1516\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 95.8337 - mae: 4.7909 - val_loss: 106.1992 - val_mae: 4.7250\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 95.5222 - mae: 4.7428 - val_loss: 105.6123 - val_mae: 4.8098\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.9685 - mae: 4.7349 - val_loss: 105.3522 - val_mae: 5.1200\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 95.0233 - mae: 4.7547 - val_loss: 105.0076 - val_mae: 4.7550\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 94.2558 - mae: 4.7198 - val_loss: 104.4303 - val_mae: 4.7555\n",
      "Epoch 15/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.8002 - mae: 4.7430 - val_loss: 104.0477 - val_mae: 4.7442\n",
      "Epoch 16/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 93.4544 - mae: 4.7041 - val_loss: 103.4828 - val_mae: 4.8367\n",
      "Epoch 17/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 93.3991 - mae: 4.7211 - val_loss: 103.2224 - val_mae: 4.9216\n",
      "Epoch 18/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 93.3223 - mae: 4.6891 - val_loss: 102.9696 - val_mae: 4.8501\n",
      "Epoch 19/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 92.6361 - mae: 4.6915 - val_loss: 102.8396 - val_mae: 4.7297\n",
      "Epoch 20/30\n",
      "162/164 [============================>.] - ETA: 0s - loss: 92.4841 - mae: 4.6806Restoring model weights from the end of the best epoch: 10.\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 92.7997 - mae: 4.6859 - val_loss: 102.9367 - val_mae: 4.9849\n",
      "Epoch 20: early stopping\n",
      "\n",
      "\n",
      "MAE: 4.724965\n",
      "MedAE: 3.075507\n",
      "r2: 0.031528\n",
      "MSE: 106.199204\n",
      "maxrror: 95.594521\n",
      "Concluido 7\n",
      "Found 89905 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20982, 300)\n",
      "Shape of data test tensor: (2331, 300)\n",
      "train_y: (20982, 1)\n",
      "test_y: (2331, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_7 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 10s 31ms/step - loss: 113.9789 - mae: 4.8923 - val_loss: 99.5832 - val_mae: 4.4633\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 28ms/step - loss: 101.1511 - mae: 4.7290 - val_loss: 96.9603 - val_mae: 4.7233\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 28ms/step - loss: 99.5583 - mae: 4.7875 - val_loss: 96.2460 - val_mae: 4.8560\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 98.8245 - mae: 4.8121 - val_loss: 95.7886 - val_mae: 4.9213\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 29ms/step - loss: 98.4434 - mae: 4.8168 - val_loss: 95.2217 - val_mae: 4.6766\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 28ms/step - loss: 98.1021 - mae: 4.8152 - val_loss: 94.9777 - val_mae: 4.6396\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 28ms/step - loss: 97.7273 - mae: 4.8143 - val_loss: 94.6234 - val_mae: 4.8479\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 28ms/step - loss: 97.2946 - mae: 4.7830 - val_loss: 94.0620 - val_mae: 4.7683\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 28ms/step - loss: 97.0595 - mae: 4.7953 - val_loss: 93.6014 - val_mae: 4.6892\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.4517 - mae: 4.7809 - val_loss: 93.2008 - val_mae: 4.5634\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 96.0885 - mae: 4.7682Restoring model weights from the end of the best epoch: 1.\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 96.0885 - mae: 4.7682 - val_loss: 92.6222 - val_mae: 4.7425\n",
      "Epoch 11: early stopping\n",
      "\n",
      "\n",
      "MAE: 4.463297\n",
      "MedAE: 2.732204\n",
      "r2: -0.020435\n",
      "MSE: 99.583169\n",
      "maxrror: 96.739255\n",
      "Concluido 8\n",
      "Found 89590 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20982, 300)\n",
      "Shape of data test tensor: (2331, 300)\n",
      "train_y: (20982, 1)\n",
      "test_y: (2331, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_8 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 9s 30ms/step - loss: 115.0912 - mae: 4.9357 - val_loss: 80.3187 - val_mae: 4.3799\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 102.6869 - mae: 4.7731 - val_loss: 78.6339 - val_mae: 4.6929\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 4s 27ms/step - loss: 101.5682 - mae: 4.8549 - val_loss: 78.0716 - val_mae: 4.5128\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 4s 27ms/step - loss: 100.9169 - mae: 4.8399 - val_loss: 77.9552 - val_mae: 4.5684\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 4s 27ms/step - loss: 100.3945 - mae: 4.8614 - val_loss: 78.1983 - val_mae: 4.6778\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 100.0882 - mae: 4.8276 - val_loss: 78.7640 - val_mae: 4.9070\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 99.7289 - mae: 4.8645 - val_loss: 77.6203 - val_mae: 4.5944\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 4s 27ms/step - loss: 99.1812 - mae: 4.8259 - val_loss: 77.1942 - val_mae: 4.5101\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 98.9991 - mae: 4.8065 - val_loss: 77.8455 - val_mae: 4.8418\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 98.6203 - mae: 4.8118 - val_loss: 77.2203 - val_mae: 4.7317\n",
      "Epoch 11/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 98.2314 - mae: 4.8114Restoring model weights from the end of the best epoch: 1.\n",
      "164/164 [==============================] - 4s 27ms/step - loss: 98.4022 - mae: 4.8131 - val_loss: 76.7030 - val_mae: 4.5879\n",
      "Epoch 11: early stopping\n",
      "\n",
      "\n",
      "MAE: 4.379928\n",
      "MedAE: 2.987393\n",
      "r2: -0.017799\n",
      "MSE: 80.318743\n",
      "maxrror: 95.217103\n",
      "Concluido 9\n",
      "Found 89471 unique tokens.\n",
      "Vocab_size: 96953\n",
      "Shape of data tensor: (20982, 300)\n",
      "Shape of data test tensor: (2331, 300)\n",
      "train_y: (20982, 1)\n",
      "test_y: (2331, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 300)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 300, 300)          29085900  \n",
      "                                                                 \n",
      " average_pooling1d_9 (Avera  (None, 1, 300)            0         \n",
      " gePooling1D)                                                    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29159171 (111.23 MB)\n",
      "Trainable params: 73271 (286.21 KB)\n",
      "Non-trainable params: 29085900 (110.95 MB)\n",
      "_________________________________________________________________\n",
      "Modelo compilado...\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 9s 29ms/step - loss: 113.1269 - mae: 4.8996 - val_loss: 96.4653 - val_mae: 4.5312\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 100.8256 - mae: 4.7921 - val_loss: 94.1764 - val_mae: 4.6106\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 99.7617 - mae: 4.8246 - val_loss: 93.5912 - val_mae: 4.6181\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 99.0543 - mae: 4.8510 - val_loss: 92.8239 - val_mae: 4.4053\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 98.7127 - mae: 4.8375 - val_loss: 92.4171 - val_mae: 4.4328\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 98.1678 - mae: 4.8140 - val_loss: 92.9824 - val_mae: 4.8623\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 97.9975 - mae: 4.8322 - val_loss: 92.8557 - val_mae: 4.8306\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 97.5330 - mae: 4.8055 - val_loss: 91.4338 - val_mae: 4.4868\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 97.3818 - mae: 4.8172 - val_loss: 91.2643 - val_mae: 4.4686\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 96.6897 - mae: 4.7938 - val_loss: 92.1377 - val_mae: 4.9214\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 96.5762 - mae: 4.7894 - val_loss: 90.8195 - val_mae: 4.6835\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 95.7555 - mae: 4.7787 - val_loss: 90.2893 - val_mae: 4.5452\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - 4s 27ms/step - loss: 95.7677 - mae: 4.7642 - val_loss: 90.0252 - val_mae: 4.4474\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 95.2748 - mae: 4.7721Restoring model weights from the end of the best epoch: 4.\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 95.2748 - mae: 4.7721 - val_loss: 89.7406 - val_mae: 4.4570\n",
      "Epoch 14: early stopping\n",
      "\n",
      "\n",
      "MAE: 4.405268\n",
      "MedAE: 3.105010\n",
      "r2: 0.019408\n",
      "MSE: 92.823858\n",
      "maxrror: 94.335944\n",
      "Concluido 10\n"
     ]
    }
   ],
   "source": [
    "vetMAE = []\n",
    "vetR2 = []\n",
    "vetMSE = []\n",
    "vetMdae = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "\n",
    "    x_train, test_x = X[train_index], X[test_index]\n",
    "    train_y, test_y = y[train_index], y[test_index]\n",
    "    \n",
    "    # get the raw text data\n",
    "    texts_train = x_train.astype(str)\n",
    "    texts_test = test_x.astype(str)\n",
    "    # vectorize the text samples                                   \n",
    "    tokenizer = Tokenizer(num_words = MAX_LEN, char_level=False, lower=False) \n",
    "    tokenizer.fit_on_texts(texts_train)                            \n",
    "    encSequences = tokenizer.texts_to_sequences(texts_train)          \n",
    "    encSequences_test = tokenizer.texts_to_sequences(texts_test)      \n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    #vocab_size = len(tokenizer.word_index) + 1 \n",
    "    print('Vocab_size: '+ str(vocab_size))\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 300 #number of words in each text\n",
    "\n",
    "    x_train = pad_sequences(encSequences, maxlen= MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    x_test = pad_sequences(encSequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    print('Shape of data tensor:', x_train.shape)\n",
    "    print('Shape of data test tensor:', x_test.shape)\n",
    "\n",
    "    print('train_y: ' + str(train_y.shape))\n",
    "    print('test_y: ' + str(test_y.shape))\n",
    "\n",
    "\n",
    "    #Sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(vocab_size,300,weights = [embedding_matrix],input_length=300,trainable = False))\n",
    "    \n",
    "    model.add(AveragePooling1D(pool_size=300))\n",
    "    \n",
    "    print(model.output_shape)\n",
    "    \n",
    "    #Enable this line for the model with LSTM\n",
    "    model.add(LSTM(50, dropout=0.3, recurrent_dropout=0.2, return_sequences=False)) \n",
    "    \n",
    "    #Disable the line below for the model with the LSTM layer \n",
    "    #model.add(Flatten()) \n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu')) \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    adam = Adam(lr = 0.001, beta_1 = 0.99, beta_2 = 0.999, epsilon = None, amsgrad = False)\n",
    "\n",
    "    model.compile(loss = 'mse', optimizer= 'adam', metrics=['mae'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    print('Modelo compilado...')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "    model_history = model.fit(x_train, train_y,\n",
    "              batch_size= 128,\n",
    "              epochs=30, callbacks=[es],\n",
    "              validation_data=(x_test, test_y))                \n",
    "\n",
    "    y_pred = model.predict(x_test, batch_size=None, verbose=0, steps=None)\n",
    "    x_pred = model.predict(x_train, batch_size=None, verbose=0, steps=None)\n",
    "\n",
    "\n",
    "    #Metrics\n",
    "    print(\"\\n\")\n",
    "    mae = mean_absolute_error(test_y, y_pred)\n",
    "    vetMAE.append(mae)\n",
    "    print(\"MAE: %f\" % (mae))\n",
    "    medAE = median_absolute_error(test_y, y_pred)\n",
    "    vetMdae.append(medAE)\n",
    "    print(\"MedAE: %f\" % (medAE))\n",
    "    r2 = r2_score(test_y, y_pred, multioutput='raw_values')\n",
    "    vetR2.append(r2)\n",
    "    print(\"r2: %f\" % (r2))\n",
    "    mse = mean_squared_error(test_y, y_pred)\n",
    "    vetMSE.append(mse)\n",
    "    print(\"MSE: %f\" % (mse))\n",
    "    mErr = max_error(test_y, y_pred)\n",
    "    print(\"maxrror: %f\" % (mErr))\n",
    "\n",
    "    i = i + 1\n",
    "    print(\"Concluido \" + str(i))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b375048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maeMedio = np.mean(vetMAE)  \n",
    "madAEMedio = np.mean(vetMdae)  \n",
    "r2Medio = np.mean(vetR2)  \n",
    "mseMedio = np.mean(vetMSE) \n",
    "stdMae = np.std(vetMAE)\n",
    "stdr2 = np.std(vetR2)\n",
    "stdMse = np.std(vetMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c7cd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maeMedio: 4.549541827277139\n",
      "madAEMedio: 2.87524573802948\n",
      "r2Medio: 0.01912670061722177\n",
      "mseMedio: 98.02318797246164\n",
      "stdMae: 0.187942159207074\n",
      "stdr2: 0.046368730743290384\n",
      "stdMse: 9.08577432730454\n"
     ]
    }
   ],
   "source": [
    "print('maeMedio: ' + str(maeMedio))\n",
    "print('madAEMedio: ' + str(madAEMedio))\n",
    "print('r2Medio: ' + str(r2Medio))\n",
    "print('mseMedio: ' + str(mseMedio))\n",
    "print('stdMae: ' + str(stdMae))\n",
    "print('stdr2: ' + str(stdr2))\n",
    "print('stdMse: ' + str(stdMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5b75ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.4082985],\n",
       "       [4.6896725],\n",
       "       [4.933695 ],\n",
       "       ...,\n",
       "       [6.0474453],\n",
       "       [4.0168257],\n",
       "       [5.9728684]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a4fab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "Ytest_ = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d1f5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE - Min: 4.144141502617664, Max: 4.792056991211696, Média: 4.549541827277139\n",
      "MedAE - Min: 2.1240153312683105, Max: 3.2041966915130615, Média: 2.87524573802948\n",
      "R2 - Min: -0.04141464058328048, Max: 0.09306307362604538, Média: 0.01912670061722177\n",
      "MSE - Min: 80.31874340268091, Max: 109.40815017075299, Média: 98.02318797246164\n"
     ]
    }
   ],
   "source": [
    "def calcular_estatisticas(vetor, nome):\n",
    "    minimo = np.min(vetor)\n",
    "    maximo = np.max(vetor)\n",
    "    media = np.mean(vetor)\n",
    "    print(f\"{nome} - Min: {minimo}, Max: {maximo}, Média: {media}\")\n",
    "\n",
    "# Exibindo as estatísticas para cada métrica\n",
    "calcular_estatisticas(vetMAE, \"MAE\")\n",
    "calcular_estatisticas(vetMdae, \"MedAE\")\n",
    "calcular_estatisticas(vetR2, \"R2\")\n",
    "calcular_estatisticas(vetMSE, \"MSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d418ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vetMAE.min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197e53d",
   "metadata": {},
   "source": [
    "### Gerando o gráfico de perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1514ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98467813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8cdc76f0d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ce1e613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"Loss\"\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286136c",
   "metadata": {},
   "source": [
    "### Implementando o teste wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "815d8955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatística do teste: [862777.]\n",
      "Valor p: [1.24368054e-52]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon# Realizando o teste de Wilcoxon\n",
    "statistic, p_value = wilcoxon(test_y, y_pred)\n",
    "\n",
    "# Imprimindo o valor estatístico e o valor p\n",
    "print(\"Estatística do teste:\", statistic)\n",
    "print(\"Valor p:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "155a839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatística do teste: 0.0\n",
      "Valor p: 0.5\n"
     ]
    }
   ],
   "source": [
    "#avaliando o MAE dos dois modelos \n",
    "glove = [4.79, 4.14]\n",
    "bert = [4.52, 0.09]\n",
    "# Realizando o teste de Wilcoxon\n",
    "statistic, p_value = wilcoxon(glove, bert)\n",
    "\n",
    "# Imprimindo o valor estatístico e o valor p\n",
    "print(\"Estatística do teste:\", statistic)\n",
    "print(\"Valor p:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe0615",
   "metadata": {},
   "source": [
    "Analyzing the statistics regarding the evaluation metrics of the two models compared here, BERT and GloVe, it is possible to conclude that there is no significant difference between the two samples, thus proving that the results of the two models are very similar.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
